{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd0ae33f-2c33-4f55-9218-a184f44ea8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_action_path = os.path.join('train_action_history.csv')\n",
    "train_cdna_path   = os.path.join('train_cdna_data.csv')\n",
    "test_action_path  = os.path.join('test_action_history.csv')\n",
    "test_cdna_path    = os.path.join('test_cdna_data.csv')\n",
    "test_customers_path = os.path.join('test_customers.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc02053-bc54-4da6-9241-cfe07bf05f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85136aba-c7fa-4420-b730-cbd29959f839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ishaan\\AppData\\Local\\Temp\\ipykernel_22148\\584636974.py:9: DtypeWarning: Columns (7,12,13,15,16,17,18,19,29,37,45,46,47,56,58,66,69,71,74,228) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_cdna = pd.read_csv(train_cdna_path)  # parse_dates if you have a cDNA snapshot date column\n",
      "C:\\Users\\Ishaan\\AppData\\Local\\Temp\\ipykernel_22148\\584636974.py:16: DtypeWarning: Columns (15,16,17,18,19,45,46,47,56,58,71,228) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_cdna = pd.read_csv(test_cdna_path)\n"
     ]
    }
   ],
   "source": [
    "# 1) Load train_action_history\n",
    "train_action = pd.read_csv(\n",
    "    train_action_path,\n",
    "    parse_dates=['send_timestamp', 'open_timestamp'],  # parse timestamps\n",
    "    # low_memory=True can help if memory is limited\n",
    ")\n",
    "\n",
    "# 2) Load train_cdna_data\n",
    "train_cdna = pd.read_csv(train_cdna_path)  # parse_dates if you have a cDNA snapshot date column\n",
    "\n",
    "# 3) Similarly for test datasets\n",
    "test_action = pd.read_csv(\n",
    "    test_action_path,\n",
    "    parse_dates=['send_timestamp', 'open_timestamp'],\n",
    ")\n",
    "test_cdna = pd.read_csv(test_cdna_path)\n",
    "test_customers = pd.read_csv(test_customers_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a4acc3-84af-420f-851c-a696c61acf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of exact duplicates in train_action: 0\n"
     ]
    }
   ],
   "source": [
    "# Check if (customer_code, batch_id, offer_subid) might have duplicates\n",
    "duplicates = train_action.duplicated(subset=['customer_code', 'batch_id', 'Offer_subid'])\n",
    "print(\"Number of exact duplicates in train_action:\", duplicates.sum())\n",
    "\n",
    "# If any duplicates exist, drop them\n",
    "if duplicates.sum() > 0:\n",
    "    train_action = train_action.drop_duplicates(subset=['customer_code', 'batch_id', 'Offer_subid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d1855bc-b173-496b-a4c5-44deea871043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid (send > open) rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Ensure send_timestamp is always <= open_timestamp (if open_timestamp is not null)\n",
    "invalid_mask = (train_action['open_timestamp'].notna()) & (train_action['send_timestamp'] > train_action['open_timestamp'])\n",
    "print(\"Invalid (send > open) rows:\", invalid_mask.sum())\n",
    "\n",
    "# Potentially drop or fix these rows if needed\n",
    "train_action = train_action[~invalid_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58b662f7-ed78-4fae-ab5c-b890243735cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_action['is_open'] = train_action['open_timestamp'].notnull().astype(int)\n",
    "test_action['is_open'] = test_action['open_timestamp'].notnull().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f655aa3-3255-4143-a017-10c2b9b396ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'send_timestamp' is in datetime format\n",
    "train_action['send_timestamp'] = pd.to_datetime(train_action['send_timestamp'], errors='coerce')\n",
    "\n",
    "# Now you can extract the day_of_week and hour\n",
    "train_action['day_of_week'] = train_action['send_timestamp'].dt.weekday  # Monday=0, Sunday=6\n",
    "train_action['hour'] = train_action['send_timestamp'].dt.hour\n",
    "\n",
    "\n",
    "# Ensure 'send_timestamp' is in datetime format\n",
    "test_action['send_timestamp'] = pd.to_datetime(test_action['send_timestamp'], errors='coerce')\n",
    "\n",
    "# Now you can extract the day_of_week and hour\n",
    "test_action['day_of_week'] = test_action['send_timestamp'].dt.weekday  # Monday=0, Sunday=6\n",
    "test_action['hour'] = test_action['send_timestamp'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7a9ca59-61c5-4f89-8a62-66890f127462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slot_number_for_send(timestamp):\n",
    "    \"\"\"\n",
    "    Returns slot_1 to slot_28 based on:\n",
    "      - day_of_week: 0..6 (Mon..Sun)\n",
    "      - hour: from 9..21 in 3-hour increments\n",
    "    Returns None if outside 9:00-21:00\n",
    "    \"\"\"\n",
    "    day = timestamp.weekday()  # 0=Mon, 6=Sun\n",
    "    hour = timestamp.hour\n",
    "\n",
    "    # If hour is outside [9..20], it doesn't map to a valid 9AM-9PM slot\n",
    "    # (21 is 9PM boundary, so hour=21 is outside).\n",
    "    if hour < 9 or hour >= 21:\n",
    "        return None\n",
    "\n",
    "    # Within 9..20, figure out which 3-hour block:\n",
    "    # 9-11 -> block 0\n",
    "    # 12-14 -> block 1\n",
    "    # 15-17 -> block 2\n",
    "    # 18-20 -> block 3\n",
    "    block = (hour - 9) // 3  # integer division\n",
    "\n",
    "    # Each day has 4 blocks, so overall slot = day*4 + block + 1\n",
    "    slot_index = day*4 + block + 1\n",
    "    return f\"slot_{slot_index}\"\n",
    "\n",
    "train_action['send_slot'] = train_action['send_timestamp'].apply(get_slot_number_for_send)\n",
    "test_action['send_slot'] = test_action['send_timestamp'].apply(get_slot_number_for_send)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa262ccf-5e98-4a88-968d-15e194192bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slot_number_for_open(ts):\n",
    "    if pd.isna(ts):\n",
    "        return None\n",
    "    day = ts.weekday()\n",
    "    hour = ts.hour\n",
    "    if hour < 9 or hour >= 21:\n",
    "        return None\n",
    "    block = (hour - 9) // 3\n",
    "    slot_index = day * 4 + block + 1\n",
    "    return f\"slot_{slot_index}\"\n",
    "\n",
    "train_action['open_slot'] = train_action['open_timestamp'].apply(get_slot_number_for_open)\n",
    "test_action['open_slot'] = test_action['open_timestamp'].apply(get_slot_number_for_open)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29938094-8860-45ca-a2fc-4f91bef124cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slot_number_for_send_ML(ts):\n",
    "    if pd.isna(ts):\n",
    "        return 0\n",
    "    day = ts.weekday()\n",
    "    hour = ts.hour\n",
    "    if hour < 9 or hour >= 21:\n",
    "        return 0\n",
    "    block = (hour - 9) // 3\n",
    "    slot_index = day * 4 + block + 1\n",
    "    return int(slot_index)\n",
    "\n",
    "train_action['send_slot_ML'] = train_action['send_timestamp'].apply(get_slot_number_for_send_ML)\n",
    "test_action['send_slot_ML'] = test_action['send_timestamp'].apply(get_slot_number_for_send_ML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d9758db-3c6f-4bca-9293-1f7a1e5444a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slot_number_for_open_ML(ts):\n",
    "    if pd.isna(ts):\n",
    "        return 0\n",
    "    day = ts.weekday()\n",
    "    hour = ts.hour\n",
    "    if hour < 9 or hour >= 21:\n",
    "        return 0\n",
    "    block = (hour - 9) // 3\n",
    "    slot_index = day * 4 + block + 1\n",
    "    return int(slot_index)\n",
    "\n",
    "train_action['open_slot_ML'] = train_action['open_timestamp'].apply(get_slot_number_for_open_ML)\n",
    "test_action['open_slot_ML'] = test_action['open_timestamp'].apply(get_slot_number_for_open_ML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f0522db-df1a-4b33-97c6-bef25d95c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer-level engagement metrics\n",
    "train_action['open_delay'] = (train_action['open_timestamp'] - train_action['send_timestamp']).dt.total_seconds()\n",
    "\n",
    "customer_aggs = train_action.groupby('customer_code').agg(\n",
    "    open_rate=('is_open', 'mean'),\n",
    "    total_opens=('is_open', 'sum'),\n",
    "    total_emails=('is_open', 'count'),\n",
    "    unique_slots=('send_slot_ML', 'nunique'),\n",
    "    most_common_send=('send_slot_ML', lambda x: x.mode().iloc[0] if not x.empty else 0),\n",
    "    most_common_open=('open_slot_ML', lambda x: x[x > 0].mode().iloc[0] if any(x > 0) else 0),\n",
    "    avg_response_time=('open_delay', lambda x: x[x > 0].mean() if any(x > 0) else 0),\n",
    "    med_response_time=('open_delay', lambda x: x[x > 0].median() if any(x > 0) else 0)\n",
    ").reset_index()\n",
    "\n",
    "# Time-based success patterns\n",
    "hourly_stats = train_action.groupby('hour').agg(\n",
    "    hourly_count=('is_open', 'count'),\n",
    "    hourly_mean=('is_open', 'mean'),\n",
    "    hourly_sum=('is_open', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "daily_stats = train_action.groupby('day_of_week').agg(\n",
    "    daily_count=('is_open', 'count'),\n",
    "    daily_mean=('is_open', 'mean'),\n",
    "    daily_sum=('is_open', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "slot_stats = train_action.groupby('send_slot_ML').agg(\n",
    "    slot_count=('is_open', 'count'),\n",
    "    slot_mean=('is_open', 'mean'),\n",
    "    slot_sum=('is_open', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Repeat for test data\n",
    "test_action['open_delay'] = (test_action['open_timestamp'] - test_action['send_timestamp']).dt.total_seconds()\n",
    "\n",
    "test_customer_aggs = test_action.groupby('customer_code').agg(\n",
    "    open_rate=('is_open', 'mean'),\n",
    "    total_opens=('is_open', 'sum'),\n",
    "    total_emails=('is_open', 'count'),\n",
    "    unique_slots=('send_slot_ML', 'nunique'),\n",
    "    most_common_send=('send_slot_ML', lambda x: x.mode().iloc[0] if not x.empty else 0),\n",
    "    most_common_open=('open_slot_ML', lambda x: x[x > 0].mode().iloc[0] if any(x > 0) else 0),\n",
    "    avg_response_time=('open_delay', lambda x: x[x > 0].mean() if any(x > 0) else 0),\n",
    "    med_response_time=('open_delay', lambda x: x[x > 0].median() if any(x > 0) else 0)\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32897097-6a1f-4771-8562-1e9281aecf12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_code</th>\n",
       "      <th>Offer_id</th>\n",
       "      <th>Offer_subid</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>product_category</th>\n",
       "      <th>product_sub_category</th>\n",
       "      <th>send_timestamp</th>\n",
       "      <th>open_timestamp</th>\n",
       "      <th>is_open</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>send_slot</th>\n",
       "      <th>open_slot</th>\n",
       "      <th>send_slot_ML</th>\n",
       "      <th>open_slot_ML</th>\n",
       "      <th>open_delay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00199d3467a7191db5bfa4e5f9a62eeb96fb0b602c3ec5...</td>\n",
       "      <td>AC_100044882</td>\n",
       "      <td>VR_200098111</td>\n",
       "      <td>70000078</td>\n",
       "      <td>CC_ACQ_SECURED</td>\n",
       "      <td>SECURED_ACQ</td>\n",
       "      <td>2024-10-22 17:35:27+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>slot_7</td>\n",
       "      <td>None</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001f2abab1bccc25d00bba68fea57a81cab1c76d485515...</td>\n",
       "      <td>AC_100048426</td>\n",
       "      <td>VR_200108485</td>\n",
       "      <td>10000253</td>\n",
       "      <td>SIP / MF</td>\n",
       "      <td>ACQUISITION</td>\n",
       "      <td>2024-08-09 18:20:18+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>slot_20</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00298fc11fb6924004c041f141f92c3c74e209d9a737d9...</td>\n",
       "      <td>AC_100052603</td>\n",
       "      <td>VR_200127708</td>\n",
       "      <td>70000074</td>\n",
       "      <td>CC_INORGANIC</td>\n",
       "      <td>EMI</td>\n",
       "      <td>2024-09-20 10:45:20+00:00</td>\n",
       "      <td>2024-09-20 10:45:31+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>slot_17</td>\n",
       "      <td>slot_17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0038a46221c0175fc8938ebc8aef8d0f83b3ac1ad84662...</td>\n",
       "      <td>AC_100047006</td>\n",
       "      <td>VR_200105035</td>\n",
       "      <td>70000072</td>\n",
       "      <td>CC_ACQ_SECURED</td>\n",
       "      <td>SECURED_ACQ</td>\n",
       "      <td>2024-09-06 12:37:16+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>slot_18</td>\n",
       "      <td>None</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00409395a831af7fd41d0ec70a5be3bda13d64cb3e4be9...</td>\n",
       "      <td>AC_100046122</td>\n",
       "      <td>VR_200100845</td>\n",
       "      <td>10000246</td>\n",
       "      <td>RURAL</td>\n",
       "      <td>BALANCE BUILD UP</td>\n",
       "      <td>2024-06-23 11:03:13+00:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>slot_25</td>\n",
       "      <td>None</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       customer_code      Offer_id  \\\n",
       "0  00199d3467a7191db5bfa4e5f9a62eeb96fb0b602c3ec5...  AC_100044882   \n",
       "1  001f2abab1bccc25d00bba68fea57a81cab1c76d485515...  AC_100048426   \n",
       "2  00298fc11fb6924004c041f141f92c3c74e209d9a737d9...  AC_100052603   \n",
       "3  0038a46221c0175fc8938ebc8aef8d0f83b3ac1ad84662...  AC_100047006   \n",
       "4  00409395a831af7fd41d0ec70a5be3bda13d64cb3e4be9...  AC_100046122   \n",
       "\n",
       "    Offer_subid  batch_id product_category product_sub_category  \\\n",
       "0  VR_200098111  70000078   CC_ACQ_SECURED          SECURED_ACQ   \n",
       "1  VR_200108485  10000253         SIP / MF          ACQUISITION   \n",
       "2  VR_200127708  70000074     CC_INORGANIC                  EMI   \n",
       "3  VR_200105035  70000072   CC_ACQ_SECURED          SECURED_ACQ   \n",
       "4  VR_200100845  10000246            RURAL     BALANCE BUILD UP   \n",
       "\n",
       "             send_timestamp            open_timestamp  is_open  day_of_week  \\\n",
       "0 2024-10-22 17:35:27+00:00                       NaT        0            1   \n",
       "1 2024-08-09 18:20:18+00:00                       NaT        0            4   \n",
       "2 2024-09-20 10:45:20+00:00 2024-09-20 10:45:31+00:00        1            4   \n",
       "3 2024-09-06 12:37:16+00:00                       NaT        0            4   \n",
       "4 2024-06-23 11:03:13+00:00                       NaT        0            6   \n",
       "\n",
       "   hour send_slot open_slot  send_slot_ML  open_slot_ML  open_delay  \n",
       "0    17    slot_7      None             7             0         NaN  \n",
       "1    18   slot_20      None            20             0         NaN  \n",
       "2    10   slot_17   slot_17            17            17        11.0  \n",
       "3    12   slot_18      None            18             0         NaN  \n",
       "4    11   slot_25      None            25             0         NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_action.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d22cab7f-cf0e-46e9-b9f8-a2e78e2f71ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_CODE</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v294</th>\n",
       "      <th>v295</th>\n",
       "      <th>v296</th>\n",
       "      <th>v297</th>\n",
       "      <th>v298</th>\n",
       "      <th>v299</th>\n",
       "      <th>v300</th>\n",
       "      <th>v301</th>\n",
       "      <th>v302</th>\n",
       "      <th>batch_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ab617a6a0a8582f4aaa1aeda38fd73377cb911e6096a98...</td>\n",
       "      <td>50-54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>THANE</td>\n",
       "      <td>99999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>IN</td>\n",
       "      <td>...</td>\n",
       "      <td>187</td>\n",
       "      <td>44</td>\n",
       "      <td>204</td>\n",
       "      <td>150</td>\n",
       "      <td>73</td>\n",
       "      <td>271</td>\n",
       "      <td>164</td>\n",
       "      <td>170</td>\n",
       "      <td>246</td>\n",
       "      <td>2024-09-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6e8e3227297409f3f33578400302825263cadc2ed0d1a0...</td>\n",
       "      <td>35-39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZZ</td>\n",
       "      <td>Pune</td>\n",
       "      <td>ZZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>411</td>\n",
       "      <td>...</td>\n",
       "      <td>187</td>\n",
       "      <td>44</td>\n",
       "      <td>204</td>\n",
       "      <td>150</td>\n",
       "      <td>73</td>\n",
       "      <td>271</td>\n",
       "      <td>164</td>\n",
       "      <td>170</td>\n",
       "      <td>246</td>\n",
       "      <td>2024-09-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1b42f270aba756b64d7ae4e2409313097b0c91f7c2f2c7...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>BHIWANI</td>\n",
       "      <td>95013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>IN</td>\n",
       "      <td>...</td>\n",
       "      <td>187</td>\n",
       "      <td>44</td>\n",
       "      <td>204</td>\n",
       "      <td>150</td>\n",
       "      <td>73</td>\n",
       "      <td>271</td>\n",
       "      <td>164</td>\n",
       "      <td>170</td>\n",
       "      <td>246</td>\n",
       "      <td>2024-09-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06a4aae9b531a518260c7d0d88811cc202fd0d3e46d9ea...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHIKMAGALUR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>187</td>\n",
       "      <td>44</td>\n",
       "      <td>204</td>\n",
       "      <td>150</td>\n",
       "      <td>73</td>\n",
       "      <td>271</td>\n",
       "      <td>164</td>\n",
       "      <td>170</td>\n",
       "      <td>246</td>\n",
       "      <td>2024-09-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e4fa92b7a41dc019c9f40457e180e94ca60d0b5c7128e...</td>\n",
       "      <td>30-34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NASIK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>187</td>\n",
       "      <td>44</td>\n",
       "      <td>204</td>\n",
       "      <td>150</td>\n",
       "      <td>73</td>\n",
       "      <td>271</td>\n",
       "      <td>164</td>\n",
       "      <td>170</td>\n",
       "      <td>246</td>\n",
       "      <td>2024-09-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       CUSTOMER_CODE     v2  v3  v4   v5  \\\n",
       "0  ab617a6a0a8582f4aaa1aeda38fd73377cb911e6096a98...  50-54 NaN NaN   99   \n",
       "1  6e8e3227297409f3f33578400302825263cadc2ed0d1a0...  35-39 NaN NaN   ZZ   \n",
       "2  1b42f270aba756b64d7ae4e2409313097b0c91f7c2f2c7...  20-24 NaN NaN   99   \n",
       "3  06a4aae9b531a518260c7d0d88811cc202fd0d3e46d9ea...  20-24 NaN NaN  NaN   \n",
       "4  0e4fa92b7a41dc019c9f40457e180e94ca60d0b5c7128e...  30-34 NaN NaN  NaN   \n",
       "\n",
       "            v6     v7   v8     v9  v10  ... v294  v295 v296 v297 v298 v299  \\\n",
       "0        THANE  99999  NaN  INDIA   IN  ...  187    44  204  150   73  271   \n",
       "1         Pune     ZZ  NaN  INDIA  411  ...  187    44  204  150   73  271   \n",
       "2      BHIWANI  95013  NaN  INDIA   IN  ...  187    44  204  150   73  271   \n",
       "3  CHIKMAGALUR    NaN  NaN  INDIA  NaN  ...  187    44  204  150   73  271   \n",
       "4        NASIK    NaN  NaN  INDIA  NaN  ...  187    44  204  150   73  271   \n",
       "\n",
       "  v300 v301 v302  batch_date  \n",
       "0  164  170  246  2024-09-27  \n",
       "1  164  170  246  2024-09-27  \n",
       "2  164  170  246  2024-09-27  \n",
       "3  164  170  246  2024-09-27  \n",
       "4  164  170  246  2024-09-27  \n",
       "\n",
       "[5 rows x 303 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cdna.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c1fb465-3d4e-493a-8e41-28775ed10837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of null ratios:\n",
      "(-0.002, 0.1]     62\n",
      "(0.1, 0.2]         5\n",
      "(0.2, 0.3]         7\n",
      "(0.3, 0.4]        32\n",
      "(0.4, 0.5]         3\n",
      "(0.5, 0.6]        37\n",
      "(0.6, 0.7]        14\n",
      "(0.7, 0.8]        15\n",
      "(0.8, 0.9]        16\n",
      "(0.9, 1.0]       112\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Columns with <= 25.0% nulls: 68\n",
      "\n",
      "Columns with <= 50.0% nulls: 109\n",
      "\n",
      "Columns with <= 75.0% nulls: 171\n",
      "\n",
      "Columns with <= 90.0% nulls: 191\n"
     ]
    }
   ],
   "source": [
    "# Let's analyze the distribution of null ratios\n",
    "null_ratios = train_cdna.isnull().mean()\n",
    "print(\"Distribution of null ratios:\")\n",
    "print(null_ratios.value_counts(bins=10).sort_index())\n",
    "\n",
    "# Count columns at different thresholds\n",
    "thresholds = [0.25, 0.5, 0.75, 0.9]\n",
    "for t in thresholds:\n",
    "    cols = null_ratios[null_ratios <= t].index\n",
    "    print(f\"\\nColumns with <= {t*100}% nulls: {len(cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1247a88f-50e4-4db3-9bf8-9b7bea34380e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types in selected columns:\n",
      "int64      35\n",
      "object     23\n",
      "bool        9\n",
      "float64     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Select columns with <=25% nulls\n",
    "null_ratios = train_cdna.isnull().mean()\n",
    "cols_to_keep = null_ratios[null_ratios <= 0.25].index.tolist()\n",
    "\n",
    "# Let's analyze the data types of these columns\n",
    "dtypes_analysis = train_cdna[cols_to_keep].dtypes.value_counts()\n",
    "print(\"\\nData types in selected columns:\")\n",
    "print(dtypes_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65cb7a88-d968-412d-97d5-49a7c7aa5cf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column: CUSTOMER_CODE\n",
      "Unique values: 220699\n",
      "Sample values: ['ab617a6a0a8582f4aaa1aeda38fd73377cb911e6096a984784be6f123024ea50', '6e8e3227297409f3f33578400302825263cadc2ed0d1a06e9b9d0f50d94ba216', '1b42f270aba756b64d7ae4e2409313097b0c91f7c2f2c7aa8b06e89c46fc754f']\n",
      "\n",
      "Column: v2\n",
      "Unique values: 22\n",
      "Sample values: ['50-54', '35-39', '20-24']\n",
      "\n",
      "Column: v5\n",
      "Unique values: 25\n",
      "Sample values: ['99', 'ZZ', '99']\n",
      "\n",
      "Column: v6\n",
      "Unique values: 2743\n",
      "Sample values: ['THANE', 'Pune', 'BHIWANI']\n",
      "\n",
      "Column: v7\n",
      "Unique values: 208\n",
      "Sample values: ['99999', 'ZZ', '95013']\n",
      "\n",
      "Column: v8\n",
      "Unique values: 1\n",
      "Sample values: ['xxxxx', 'xxxxx', 'xxxxx']\n",
      "\n",
      "Column: v9\n",
      "Unique values: 105\n",
      "Sample values: ['INDIA', 'INDIA', 'INDIA']\n",
      "\n",
      "Column: v10\n",
      "Unique values: 210\n",
      "Sample values: ['IN', '411', 'IN']\n",
      "\n",
      "Column: v11\n",
      "Unique values: 82454\n",
      "Sample values: ['2018-08-25T00:00:00.000Z', '2018-08-03T14:12:16.000Z', '2021-10-08T00:00:00.000Z']\n",
      "\n",
      "Column: v13\n",
      "Unique values: 1\n",
      "Sample values: ['G', 'G', 'G']\n",
      "\n",
      "Column: v14\n",
      "Unique values: 1\n",
      "Sample values: ['M', 'M', 'M']\n",
      "\n",
      "Column: v15\n",
      "Unique values: 33\n",
      "Sample values: ['Resident Individuals', 'Resident Individuals', 'Resident Individuals']\n",
      "\n",
      "Column: v16\n",
      "Unique values: 2\n",
      "Sample values: ['Y', 'Y', 'Y']\n",
      "\n",
      "Column: v17\n",
      "Unique values: 2\n",
      "Sample values: ['Y', 'Y', 'Y']\n",
      "\n",
      "Column: v18\n",
      "Unique values: 2\n",
      "Sample values: ['Y', 'Y', 'Y']\n",
      "\n",
      "Column: v19\n",
      "Unique values: 2\n",
      "Sample values: ['Y', 'Y', 'Y']\n",
      "\n",
      "Column: v20\n",
      "Unique values: 2\n",
      "Sample values: ['Y', 'Y', 'Y']\n",
      "\n",
      "Column: v27\n",
      "Unique values: 11\n",
      "Sample values: ['MARRIED', 'Married', 'MARRIED']\n",
      "\n",
      "Column: v29\n",
      "Unique values: 99\n",
      "Sample values: ['SALARIED', 'Self Employed Business', 'Salaried']\n",
      "\n",
      "Column: v30\n",
      "Unique values: 83\n",
      "Sample values: ['11', '15', '95']\n",
      "\n",
      "Column: v31\n",
      "Unique values: 101\n",
      "Sample values: ['IN', 'IN', 'IN']\n",
      "\n",
      "Column: v33\n",
      "Unique values: 174552\n",
      "Sample values: ['c7e235648c651555361eb3f24ec90608fa8317b1ca8d5a37985a079f097ae2ad', '55253020c6a8f56e2208746dc08cfa4934758d7b207551cc18a8b28d289f227a', '84bcd8800fde891c698a509995ff937f796c6df2330b2eca2495c6ca213515d4']\n",
      "\n",
      "Column: v34\n",
      "Unique values: 2012\n",
      "Sample values: ['THANE', 'Pune', 'BHIWANI']\n",
      "\n",
      "Column: v35\n",
      "Unique values: 81\n",
      "Sample values: ['MAHARASHTRA', 'MAHARASHTRA', 'HARYANA']\n",
      "\n",
      "Column: v36\n",
      "Unique values: 4\n",
      "Sample values: ['MDM', 'MDM', 'MDM']\n",
      "\n",
      "Column: v37\n",
      "Unique values: 470\n",
      "Sample values: ['MAHARASHTRA', 'MAHARASHTRA', 'HARYANA']\n",
      "\n",
      "Column: v38\n",
      "Unique values: 5\n",
      "Sample values: ['GENERAL', 'GENERAL', 'H']\n",
      "\n",
      "Column: v42\n",
      "Unique values: 193\n",
      "Sample values: ['Miscallenous', 'Loans for purchase of consumer durable goods - other than staff', 'Loans for purchase of consumer durable goods - other than staff']\n",
      "\n",
      "Column: v43\n",
      "Unique values: 4\n",
      "Sample values: ['MARRIED', 'MARRIED', 'MARRIED']\n",
      "\n",
      "Column: v46\n",
      "Unique values: 1\n",
      "Sample values: ['Y', 'Y', 'Y']\n",
      "\n",
      "Column: v47\n",
      "Unique values: 1\n",
      "Sample values: ['Y', 'Y', 'Y']\n",
      "\n",
      "Column: v48\n",
      "Unique values: 1\n",
      "Sample values: ['Y', 'Y', 'Y']\n",
      "\n",
      "Column: v54\n",
      "Unique values: 11\n",
      "Sample values: ['MALE', 'Male', 'MALE']\n",
      "\n",
      "Column: v55\n",
      "Unique values: 1218\n",
      "Sample values: ['[GMAIL.COM]', '[GMAIL.COM]', '[GMAIL.COM]']\n",
      "\n",
      "Column: v56\n",
      "Unique values: 84\n",
      "Sample values: ['IN', 'IN', 'IN']\n",
      "\n",
      "Column: v57\n",
      "Unique values: 5\n",
      "Sample values: ['f32adf18d4d722a0df960cdbd1b10437f5d0b505814d2f8368d86ad62966321f', 'f32adf18d4d722a0df960cdbd1b10437f5d0b505814d2f8368d86ad62966321f', 'f32adf18d4d722a0df960cdbd1b10437f5d0b505814d2f8368d86ad62966321f']\n",
      "\n",
      "Column: v58\n",
      "Unique values: 7\n",
      "Sample values: ['N', '6', '6']\n",
      "\n",
      "Column: v59\n",
      "Unique values: 1\n",
      "Sample values: ['STANDARD', 'STANDARD', 'STANDARD']\n",
      "\n",
      "Column: v60\n",
      "Unique values: 1\n",
      "Sample values: ['N', 'N', 'N']\n",
      "\n",
      "Column: v61\n",
      "Unique values: 3\n",
      "Sample values: ['IN', 'IN', 'IN']\n",
      "\n",
      "Column: v63\n",
      "Unique values: 13\n",
      "Sample values: ['25L to 50L', '100001 to 5L', '100001 to 5L']\n",
      "\n",
      "Column: v64\n",
      "Unique values: 51\n",
      "Sample values: ['HR', 'PB', 'HR']\n",
      "\n",
      "Column: v66\n",
      "Unique values: 2\n",
      "Sample values: [True, True, True]\n",
      "\n",
      "Column: v67\n",
      "Unique values: 2\n",
      "Sample values: ['NO', 'NO', 'NO']\n",
      "\n",
      "Column: v68\n",
      "Unique values: 2\n",
      "Sample values: [False, True, False]\n",
      "\n",
      "Column: v69\n",
      "Unique values: 2\n",
      "Sample values: [True, True, True]\n",
      "\n",
      "Column: v70\n",
      "Unique values: 1\n",
      "Sample values: ['2019-07-11', '2019-07-11', '2019-07-11']\n",
      "\n",
      "Column: v71\n",
      "Unique values: 1\n",
      "Sample values: [False, False, False]\n",
      "\n",
      "Column: v72\n",
      "Unique values: 2\n",
      "Sample values: ['N', 'N', 'N']\n",
      "\n",
      "Column: v73\n",
      "Unique values: 2\n",
      "Sample values: [False, False, False]\n",
      "\n",
      "Column: v74\n",
      "Unique values: 2\n",
      "Sample values: [False, False, False]\n",
      "\n",
      "Column: v75\n",
      "Unique values: 1\n",
      "Sample values: ['ISSUED', 'ISSUED', 'ISSUED']\n",
      "\n",
      "Column: v81\n",
      "Unique values: 2\n",
      "Sample values: ['N', 'N', 'Y']\n",
      "\n",
      "Column: v99\n",
      "Unique values: 2\n",
      "Sample values: ['and', 'ios', 'and']\n",
      "\n",
      "Column: v101\n",
      "Unique values: 9\n",
      "Sample values: ['Tier 1', 'Tier 1', 'Tier 5']\n",
      "\n",
      "Column: v102\n",
      "Unique values: 8\n",
      "Sample values: ['Salaried', 'Self_Employed', 'Salaried']\n",
      "\n",
      "Column: v103\n",
      "Unique values: 2\n",
      "Sample values: [True, False, False]\n",
      "\n",
      "Column: v222\n",
      "Unique values: 506\n",
      "Sample values: ['HDFC Mid-Cap Opportunities Fund(G)', 'SBI Long Term Equity Fund-Reg(G)', 'Nippon India Small Cap Fund(G)']\n",
      "\n",
      "Column: v223\n",
      "Unique values: 614\n",
      "Sample values: ['360 ONE Flexicap Fund-Reg(G)', 'SBI Long Term Equity Fund-Reg(G)', 'ICICI Pru Value Discovery Fund(G)']\n",
      "\n",
      "Column: v229\n",
      "Unique values: 291\n",
      "Sample values: ['22101', '43552', '22102']\n",
      "\n",
      "Column: v230\n",
      "Unique values: 663\n",
      "Sample values: ['23201', '43552', '22102']\n",
      "\n",
      "Column: v271\n",
      "Unique values: 2\n",
      "Sample values: ['MORNING', 'EVENING', 'MORNING']\n",
      "\n",
      "Column: v272\n",
      "Unique values: 2\n",
      "Sample values: ['MORNING', 'MORNING', 'MORNING']\n",
      "\n",
      "Column: v273\n",
      "Unique values: 2\n",
      "Sample values: ['MORNING', 'MORNING', 'MORNING']\n",
      "\n",
      "Column: v274\n",
      "Unique values: 2\n",
      "Sample values: ['MORNING', 'MORNING', 'MORNING']\n",
      "\n",
      "Column: v275\n",
      "Unique values: 2\n",
      "Sample values: ['MORNING', 'MORNING', 'MORNING']\n",
      "\n",
      "Column: v276\n",
      "Unique values: 2\n",
      "Sample values: ['MORNING', 'MORNING', 'MORNING']\n",
      "\n",
      "Column: v277\n",
      "Unique values: 2\n",
      "Sample values: ['EVENING', 'MORNING', 'MORNING']\n",
      "\n",
      "Column: v278\n",
      "Unique values: 2\n",
      "Sample values: ['MORNING', 'MORNING', 'MORNING']\n",
      "\n",
      "Column: v279\n",
      "Unique values: 3\n",
      "Sample values: ['WEEKEND', 'WEEKDAY', 'WEEKDAY']\n",
      "\n",
      "Column: v280\n",
      "Unique values: 3\n",
      "Sample values: ['WEEKEND', 'WEEKEND', 'WEEKEND']\n",
      "\n",
      "Column: v281\n",
      "Unique values: 3\n",
      "Sample values: ['WEEKEND', 'WEEKDAY', 'WEEKDAY']\n",
      "\n",
      "Column: v282\n",
      "Unique values: 3\n",
      "Sample values: ['WEEKEND', 'WEEKDAY', 'WEEKEND']\n",
      "\n",
      "Column: v283\n",
      "Unique values: 3\n",
      "Sample values: ['WEEKEND', 'WEEKDAY', 'WEEKDAY']\n",
      "\n",
      "Column: v284\n",
      "Unique values: 3\n",
      "Sample values: ['WEEKEND', 'WEEKDAY', 'WEEKEND']\n",
      "\n",
      "Column: v285\n",
      "Unique values: 3\n",
      "Sample values: ['WEEKEND', 'WEEKDAY', 'WEEKDAY']\n",
      "\n",
      "Column: v286\n",
      "Unique values: 3\n",
      "Sample values: ['WEEKEND', 'WEEKDAY', 'WEEKEND']\n",
      "\n",
      "Column: batch_date\n",
      "Unique values: 6\n",
      "Sample values: ['2024-09-27', '2024-09-27', '2024-09-27']\n",
      "\n",
      "Column: CUSTOMER_CODE\n",
      "Unique values: 68450\n",
      "Sample values: ['635c3a7b6c89528607944fa312096e48c9f54dfaf821f50173451b5dc3f6aa67', '970ef77e5a753f82ed9ee1e4446db1dd29d237ed0f64c72647e190ec641cb974', 'ccc36bb7c74601863c7328faa90ade2d806bccea2a3c516ac36b3a011f7e0e5b']\n",
      "\n",
      "Column: v2\n",
      "Unique values: 21\n",
      "Sample values: ['40-44', '40-44', '35-39']\n",
      "\n",
      "Column: v5\n",
      "Unique values: 16\n",
      "Sample values: ['99', '99', '99']\n",
      "\n",
      "Column: v6\n",
      "Unique values: 1916\n",
      "Sample values: ['THANE', 'KOLKATA', 'KOLAR']\n",
      "\n",
      "Column: v7\n",
      "Unique values: 124\n",
      "Sample values: ['99999', '99999', '95013']\n",
      "\n",
      "Column: v8\n",
      "Unique values: 0\n",
      "Sample values: []\n",
      "\n",
      "Column: v9\n",
      "Unique values: 86\n",
      "Sample values: ['INDIA', 'INDIA', 'INDIA']\n",
      "\n",
      "Column: v10\n",
      "Unique values: 101\n",
      "Sample values: ['IN', 'IN', 'IN']\n",
      "\n",
      "Column: v11\n",
      "Unique values: 28776\n",
      "Sample values: ['2018-04-26T00:00:00.000Z', '2017-11-20T00:00:00.000Z', '2022-10-03T00:00:00.000Z']\n",
      "\n",
      "Column: v13\n",
      "Unique values: 0\n",
      "Sample values: []\n",
      "\n",
      "Column: v14\n",
      "Unique values: 0\n",
      "Sample values: []\n",
      "\n",
      "Column: v15\n",
      "Unique values: 29\n",
      "Sample values: ['Resident Individuals', 'Resident Individuals', 'Resident Individuals']\n",
      "\n",
      "Column: v16\n",
      "Unique values: 2\n",
      "Sample values: ['Y', 'Y', 'Y']\n",
      "\n",
      "Column: v17\n",
      "Unique values: 2\n",
      "Sample values: ['Y', 'Y', 'Y']\n",
      "\n",
      "Column: v18\n",
      "Unique values: 2\n",
      "Sample values: ['Y', 'Y', 'Y']\n",
      "\n",
      "Column: v19\n",
      "Unique values: 2\n",
      "Sample values: ['Y', 'Y', 'Y']\n",
      "\n",
      "Column: v20\n",
      "Unique values: 2\n",
      "Sample values: ['Y', 'Y', 'Y']\n",
      "\n",
      "Column: v27\n",
      "Unique values: 6\n",
      "Sample values: ['Married', 'MARRIED', 'Unmarried']\n",
      "\n",
      "Column: v29\n",
      "Unique values: 70\n",
      "Sample values: ['Self Employed Business', 'SALARIED', 'Salaried']\n",
      "\n",
      "Column: v30\n",
      "Unique values: 56\n",
      "Sample values: ['02', '11', '01']\n",
      "\n",
      "Column: v31\n",
      "Unique values: 84\n",
      "Sample values: ['IN', 'IN', 'IN']\n",
      "\n",
      "Column: v33\n",
      "Unique values: 52941\n",
      "Sample values: ['294cd6e2886c568796d89c66ccdaea76dbf52e44d7598d03ebd978a68c7f85fb', '678edf6b584a6a19c6cc9c684b56579c4348ae8468c9aa1f41880e29bf36fc26', '902fb49da19d011962ab0fef97a954af7e6515bfe940aab47b32a4715d4c5f27']\n",
      "\n",
      "Column: v34\n",
      "Unique values: 1386\n",
      "Sample values: ['MUMBAI', 'KOLKATA', 'CHINTAMANI']\n",
      "\n",
      "Column: v35\n",
      "Unique values: 71\n",
      "Sample values: ['MAHARASHTRA', 'WEST BENGAL', 'KARNATAKA']\n",
      "\n",
      "Column: v36\n",
      "Unique values: 3\n",
      "Sample values: ['MDM', 'MDM', 'MDM']\n",
      "\n",
      "Column: v37\n",
      "Unique values: 350\n",
      "Sample values: ['MAHARASHTRA', 'WEST BENGAL', 'KARNATAKA']\n",
      "\n",
      "Column: v38\n",
      "Unique values: 5\n",
      "Sample values: ['GENERAL', 'GENERAL', 'H']\n",
      "\n",
      "Column: v42\n",
      "Unique values: 113\n",
      "Sample values: ['Miscallenous', 'Miscallenous', 'Loans for purchase of consumer durable goods - other than staff']\n",
      "\n",
      "Column: v43\n",
      "Unique values: 3\n",
      "Sample values: ['MARRIED', 'MARRIED', 'UNMARRIED']\n",
      "\n",
      "Column: v46\n",
      "Unique values: 1\n",
      "Sample values: ['Y', 'Y', 'Y']\n",
      "\n",
      "Column: v47\n",
      "Unique values: 1\n",
      "Sample values: ['Y', 'Y', 'Y']\n",
      "\n",
      "Column: v48\n",
      "Unique values: 1\n",
      "Sample values: ['Y', 'Y', 'Y']\n",
      "\n",
      "Column: v54\n",
      "Unique values: 9\n",
      "Sample values: ['Male', 'MALE', 'Male']\n",
      "\n",
      "Column: v55\n",
      "Unique values: 686\n",
      "Sample values: ['[GMAIL.COM]', '[GMAIL.COM]', '[GMAIL.COM]']\n",
      "\n",
      "Column: v56\n",
      "Unique values: 70\n",
      "Sample values: ['IN', 'IN', 'IN']\n",
      "\n",
      "Column: v57\n",
      "Unique values: 1\n",
      "Sample values: ['24047b852f6d8d5edcd7dd877a5fe482925d82cb766fcfbf0d08b553dc10a2cf']\n",
      "\n",
      "Column: v58\n",
      "Unique values: 7\n",
      "Sample values: ['N', 'N', 'N']\n",
      "\n",
      "Column: v59\n",
      "Unique values: 1\n",
      "Sample values: ['STANDARD', 'STANDARD', 'STANDARD']\n",
      "\n",
      "Column: v60\n",
      "Unique values: 1\n",
      "Sample values: ['N', 'N', 'N']\n",
      "\n",
      "Column: v61\n",
      "Unique values: 3\n",
      "Sample values: ['356', '356', '356']\n",
      "\n",
      "Column: v63\n",
      "Unique values: 13\n",
      "Sample values: ['5L to 10L', '5L to 10L', '100001 to 5L']\n",
      "\n",
      "Column: v64\n",
      "Unique values: 48\n",
      "Sample values: ['27', '29', '03']\n",
      "\n",
      "Column: v66\n",
      "Unique values: 2\n",
      "Sample values: [True, True, True]\n",
      "\n",
      "Column: v67\n",
      "Unique values: 2\n",
      "Sample values: ['NO', 'NO', 'NO']\n",
      "\n",
      "Column: v68\n",
      "Unique values: 2\n",
      "Sample values: [True, True, False]\n",
      "\n",
      "Column: v69\n",
      "Unique values: 2\n",
      "Sample values: [True, True, True]\n",
      "\n",
      "Column: v70\n",
      "Unique values: 0\n",
      "Sample values: []\n",
      "\n",
      "Column: v71\n",
      "Unique values: 1\n",
      "Sample values: [False, False, False]\n",
      "\n",
      "Column: v72\n",
      "Unique values: 2\n",
      "Sample values: ['N', 'Y', 'N']\n",
      "\n",
      "Column: v73\n",
      "Unique values: 2\n",
      "Sample values: [False, False, False]\n",
      "\n",
      "Column: v74\n",
      "Unique values: 2\n",
      "Sample values: [False, False, False]\n",
      "\n",
      "Column: v75\n",
      "Unique values: 0\n",
      "Sample values: []\n",
      "\n",
      "Column: v81\n",
      "Unique values: 2\n",
      "Sample values: ['N', 'N', 'Y']\n",
      "\n",
      "Column: v99\n",
      "Unique values: 2\n",
      "Sample values: ['and', 'and', 'ios']\n",
      "\n",
      "Column: v101\n",
      "Unique values: 9\n",
      "Sample values: ['Tier 1', 'Tier 5', 'Tier 4']\n",
      "\n",
      "Column: v102\n",
      "Unique values: 8\n",
      "Sample values: ['Self_Employed', 'Salaried', 'Salaried']\n",
      "\n",
      "Column: v103\n",
      "Unique values: 2\n",
      "Sample values: [False, True, False]\n",
      "\n",
      "Column: v222\n",
      "Unique values: 312\n",
      "Sample values: ['UTI Value Fund-Reg(G)', 'SBI Gold-Reg(G)', 'SBI Nifty Index Fund-Reg(G)']\n",
      "\n",
      "Column: v223\n",
      "Unique values: 368\n",
      "Sample values: ['Nippon India Small Cap Fund(G)', 'SBI Gold-Reg(G)', 'SBI Nifty Index Fund-Reg(G)']\n",
      "\n",
      "Column: v229\n",
      "Unique values: 200\n",
      "Sample values: [43802.0, 43552.0, 22101.0]\n",
      "\n",
      "Column: v230\n",
      "Unique values: 401\n",
      "Sample values: ['22101', '22101', '22101']\n",
      "\n",
      "Column: v271\n",
      "Unique values: 2\n",
      "Sample values: ['MORNING', 'MORNING', 'MORNING']\n",
      "\n",
      "Column: v272\n",
      "Unique values: 2\n",
      "Sample values: ['MORNING', 'MORNING', 'MORNING']\n",
      "\n",
      "Column: v273\n",
      "Unique values: 2\n",
      "Sample values: ['MORNING', 'MORNING', 'MORNING']\n",
      "\n",
      "Column: v274\n",
      "Unique values: 2\n",
      "Sample values: ['MORNING', 'MORNING', 'MORNING']\n",
      "\n",
      "Column: v275\n",
      "Unique values: 2\n",
      "Sample values: ['MORNING', 'MORNING', 'MORNING']\n",
      "\n",
      "Column: v276\n",
      "Unique values: 2\n",
      "Sample values: ['MORNING', 'MORNING', 'MORNING']\n",
      "\n",
      "Column: v277\n",
      "Unique values: 2\n",
      "Sample values: ['MORNING', 'MORNING', 'MORNING']\n",
      "\n",
      "Column: v278\n",
      "Unique values: 2\n",
      "Sample values: ['MORNING', 'MORNING', 'MORNING']\n",
      "\n",
      "Column: v279\n",
      "Unique values: 3\n",
      "Sample values: ['WEEKDAY', 'WEEKEND', 'WEEKDAY']\n",
      "\n",
      "Column: v280\n",
      "Unique values: 3\n",
      "Sample values: ['WEEKEND', 'WEEKEND', 'WEEKEND']\n",
      "\n",
      "Column: v281\n",
      "Unique values: 3\n",
      "Sample values: ['WEEKDAY', 'WEEKEND', 'WEEKDAY']\n",
      "\n",
      "Column: v282\n",
      "Unique values: 3\n",
      "Sample values: ['WEEKEND', 'WEEKEND', 'WEEKEND']\n",
      "\n",
      "Column: v283\n",
      "Unique values: 3\n",
      "Sample values: ['WEEKDAY', 'FRIDAY', 'WEEKDAY']\n",
      "\n",
      "Column: v284\n",
      "Unique values: 3\n",
      "Sample values: ['WEEKEND', 'WEEKEND', 'WEEKEND']\n",
      "\n",
      "Column: v285\n",
      "Unique values: 3\n",
      "Sample values: ['WEEKDAY', 'WEEKDAY', 'WEEKDAY']\n",
      "\n",
      "Column: v286\n",
      "Unique values: 3\n",
      "Sample values: ['WEEKEND', 'WEEKEND', 'WEEKEND']\n",
      "\n",
      "Column: batch_date\n",
      "Unique values: 1\n",
      "Sample values: ['2024-11-29', '2024-11-29', '2024-11-29']\n"
     ]
    }
   ],
   "source": [
    "# Check object columns content\n",
    "object_cols = train_cdna.select_dtypes(include=['object']).columns\n",
    "for col in object_cols:\n",
    "    unique_vals = train_cdna[col].nunique()\n",
    "    sample_vals = train_cdna[col].dropna().head(3).tolist()\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(f\"Unique values: {unique_vals}\")\n",
    "    print(f\"Sample values: {sample_vals}\")\n",
    "\n",
    "object_cols_test = test_cdna.select_dtypes(include=['object']).columns\n",
    "for col in object_cols:\n",
    "    unique_vals = test_cdna[col].nunique()\n",
    "    sample_vals = test_cdna[col].dropna().head(3).tolist()\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(f\"Unique values: {unique_vals}\")\n",
    "    print(f\"Sample values: {sample_vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a487821d-335d-4a31-8139-573fed014fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.strategies = {}\n",
    "        self.fill_values = {}\n",
    "        self.age_medians = None\n",
    "        self.city_tiers = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Age imputation\n",
    "        mask_train = X['v2'].notna() & X['v80'].notna()\n",
    "        df_train_valid = X.loc[mask_train, ['v2', 'v80']]\n",
    "        self.age_medians = df_train_valid.groupby('v2')['v80'].median()\n",
    "        self.overall_median_v80 = X['v80'].median(skipna=True)\n",
    "\n",
    "        # Normalize and impute cities/tiers\n",
    "        X['v6'] = X['v6'].str.upper()\n",
    "        mask = X['v6'].notna() & X['v101'].notna()\n",
    "        df_valid = X.loc[mask, ['v6', 'v101']]\n",
    "        mode_df = df_valid.groupby('v6')['v101'].agg(lambda x: x.value_counts().index[0])\n",
    "        self.city_tiers = dict(zip(mode_df.index, mode_df.values))\n",
    "        self.fallback_tier = X['v101'].dropna().mode()[0]\n",
    "\n",
    "        # Normalize gender\n",
    "        X['v54'] = X['v54'].apply(lambda x: 'MALE' if str(x).upper() in ['M', 'MALE'] else 'FEMALE' if str(x).upper() in ['F', 'FEMALE'] else x)\n",
    "\n",
    "        # Normalize country\n",
    "        X['v9'] = X['v9'].apply(lambda x: 'INDIA' if str(x).upper() in ['IN', 'IN - INDIA', 'INDIA'] else x)\n",
    "\n",
    "        # Regular imputation\n",
    "        self.columns_to_drop = ['v2', 'v27', 'v29', 'v31']\n",
    "        X_filtered = X.drop(columns=self.columns_to_drop, errors='ignore')\n",
    "        \n",
    "        for column in X_filtered.columns:\n",
    "            if column not in ['v80', 'v101']:\n",
    "                if X_filtered[column].dtype == 'object':\n",
    "                    if X_filtered[column].nunique() > 100:\n",
    "                        self.strategies[column] = 'constant'\n",
    "                        self.fill_values[column] = 'MISSING'\n",
    "                    else:\n",
    "                        self.strategies[column] = 'mode'\n",
    "                        self.fill_values[column] = X_filtered[column].mode()[0]\n",
    "                elif X_filtered[column].dtype == 'bool':\n",
    "                    self.strategies[column] = 'mode'\n",
    "                    self.fill_values[column] = X_filtered[column].mode()[0]\n",
    "                elif X_filtered[column].dtype in ['int64', 'float64']:\n",
    "                    if X_filtered[column].nunique() > 20:\n",
    "                        self.strategies[column] = 'median'\n",
    "                        self.fill_values[column] = X_filtered[column].median()\n",
    "                    else:\n",
    "                        self.strategies[column] = 'mode'\n",
    "                        self.fill_values[column] = X_filtered[column].mode()[0]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Age imputation using v2 before dropping it\n",
    "        X_copy['v80'] = X_copy.apply(lambda row: self.age_medians[row['v2']] if pd.notna(row['v2']) and row['v2'] in self.age_medians.index else self.overall_median_v80 if pd.isna(row['v80']) else row['v80'], axis=1)\n",
    "        X_copy['v80'] = X_copy['v80'].astype(int)\n",
    "\n",
    "        # Normalize and impute\n",
    "        X_copy['v6'] = X_copy['v6'].str.upper()\n",
    "        X_copy['v54'] = X_copy['v54'].apply(lambda x: 'MALE' if str(x).upper() in ['M', 'MALE'] else 'FEMALE' if str(x).upper() in ['F', 'FEMALE'] else x)\n",
    "        X_copy['v9'] = X_copy['v9'].apply(lambda x: 'INDIA' if str(x).upper() in ['IN', 'IN - INDIA', 'INDIA'] else x)\n",
    "        \n",
    "        # City-tier imputation\n",
    "        X_copy['v101'] = X_copy.apply(lambda row: self.city_tiers.get(row['v6'], self.fallback_tier) if pd.isna(row['v101']) else row['v101'], axis=1)\n",
    "\n",
    "        # Drop columns after using them for imputation\n",
    "        X_copy = X_copy.drop(columns=self.columns_to_drop, errors='ignore')\n",
    "\n",
    "        # Regular imputation\n",
    "        for column in X_copy.columns:\n",
    "            if column not in ['v80', 'v101']:\n",
    "                X_copy[column].fillna(self.fill_values.get(column, 'MISSING'), inplace=True)\n",
    "\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7b8763c-5cf7-4e55-8fcb-94485d43c769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ishaan\\AppData\\Local\\Temp\\ipykernel_22148\\2356013800.py:75: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[column].fillna(self.fill_values.get(column, 'MISSING'), inplace=True)\n",
      "C:\\Users\\Ishaan\\AppData\\Local\\Temp\\ipykernel_22148\\2356013800.py:75: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_copy[column].fillna(self.fill_values.get(column, 'MISSING'), inplace=True)\n",
      "C:\\Users\\Ishaan\\AppData\\Local\\Temp\\ipykernel_22148\\2356013800.py:75: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_copy[column].fillna(self.fill_values.get(column, 'MISSING'), inplace=True)\n",
      "C:\\Users\\Ishaan\\AppData\\Local\\Temp\\ipykernel_22148\\2356013800.py:75: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_copy[column].fillna(self.fill_values.get(column, 'MISSING'), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls in train after imputation: 0\n",
      "Nulls in test after imputation: 0\n",
      "\n",
      "Shape of data:\n",
      "Train CDNA - Original: (1285402, 303), After imputation: (1285402, 64)\n",
      "Test CDNA - Original: (68450, 303), After imputation: (68450, 64)\n",
      "\n",
      "Sample of imputed data:\n",
      "                                       CUSTOMER_CODE           v6     v9  \\\n",
      "0  ab617a6a0a8582f4aaa1aeda38fd73377cb911e6096a98...        THANE  INDIA   \n",
      "1  6e8e3227297409f3f33578400302825263cadc2ed0d1a0...         PUNE  INDIA   \n",
      "2  1b42f270aba756b64d7ae4e2409313097b0c91f7c2f2c7...      BHIWANI  INDIA   \n",
      "3  06a4aae9b531a518260c7d0d88811cc202fd0d3e46d9ea...  CHIKMAGALUR  INDIA   \n",
      "4  0e4fa92b7a41dc019c9f40457e180e94ca60d0b5c7128e...        NASIK  INDIA   \n",
      "\n",
      "       v10                       v11 v30  \\\n",
      "0       IN  2018-08-25T00:00:00.000Z  11   \n",
      "1      411  2018-08-03T14:12:16.000Z  15   \n",
      "2       IN  2021-10-08T00:00:00.000Z  95   \n",
      "3  MISSING  2024-03-18T00:00:00.000Z  01   \n",
      "4  MISSING  2024-02-26T00:00:00.000Z  04   \n",
      "\n",
      "                                                 v33      v34          v35  \\\n",
      "0  c7e235648c651555361eb3f24ec90608fa8317b1ca8d5a...    THANE  MAHARASHTRA   \n",
      "1  55253020c6a8f56e2208746dc08cfa4934758d7b207551...     Pune  MAHARASHTRA   \n",
      "2  84bcd8800fde891c698a509995ff937f796c6df2330b2e...  BHIWANI      HARYANA   \n",
      "3                                            MISSING  MISSING  MAHARASHTRA   \n",
      "4                                            MISSING  MISSING  MAHARASHTRA   \n",
      "\n",
      "   v36  ... v294  v295  v296  v297 v298  v299  v300  v301  v302  batch_date  \n",
      "0  MDM  ...  187    44   204   150   73   271   164   170   246  2024-09-27  \n",
      "1  MDM  ...  187    44   204   150   73   271   164   170   246  2024-09-27  \n",
      "2  MDM  ...  187    44   204   150   73   271   164   170   246  2024-09-27  \n",
      "3  MDM  ...  187    44   204   150   73   271   164   170   246  2024-09-27  \n",
      "4  MDM  ...  187    44   204   150   73   271   164   170   246  2024-09-27  \n",
      "\n",
      "[5 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "# First, let's keep only columns with <=50% nulls\n",
    "null_ratios = train_cdna.isnull().mean()\n",
    "cols_to_keep = null_ratios[null_ratios <= 0.25].index.tolist()\n",
    "\n",
    "# Filter columns\n",
    "train_cdna_filtered = train_cdna[cols_to_keep].copy()\n",
    "test_cdna_filtered = test_cdna[cols_to_keep].copy()\n",
    "\n",
    "# Initialize and fit the imputer\n",
    "imputer = CustomImputer()\n",
    "imputer = imputer.fit(train_cdna_filtered)\n",
    "\n",
    "# Transform both train and test data\n",
    "train_cdna_imputed = imputer.transform(train_cdna_filtered)\n",
    "test_cdna_imputed = imputer.transform(test_cdna_filtered)\n",
    "\n",
    "# Verify no nulls remain\n",
    "print(\"Nulls in train after imputation:\", train_cdna_imputed.isnull().sum().sum())\n",
    "print(\"Nulls in test after imputation:\", test_cdna_imputed.isnull().sum().sum())\n",
    "\n",
    "# Show shape of data before and after\n",
    "print(\"\\nShape of data:\")\n",
    "print(f\"Train CDNA - Original: {train_cdna.shape}, After imputation: {train_cdna_imputed.shape}\")\n",
    "print(f\"Test CDNA - Original: {test_cdna.shape}, After imputation: {test_cdna_imputed.shape}\")\n",
    "\n",
    "# Show sample of imputed data\n",
    "print(\"\\nSample of imputed data:\")\n",
    "print(train_cdna_imputed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24c28d04-e142-40d1-9725-55ce0ce7ca70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train merged shape: (8797911, 81)\n",
      "Test merged shape: (1456503, 80)\n",
      "Train Validation:\n",
      "Original shapes - Action: (8797911, 17), CDNA: (1285402, 64)\n",
      "Merged shape: (8797911, 81)\n",
      "Missing values in merged: 20923384\n",
      "Unique customers in action: 221204\n",
      "Unique customers in merged: 221204\n",
      "\n",
      "Test Validation:\n",
      "Original shapes - Action: (1456503, 16), CDNA: (68450, 64)\n",
      "Merged shape: (1456503, 80)\n",
      "Missing values in merged: 2523063\n",
      "Unique customers in action: 64700\n",
      "Unique customers in merged: 64700\n"
     ]
    }
   ],
   "source": [
    "# Fix column names first\n",
    "train_action['customer_code'] = train_action['customer_code']\n",
    "train_cdna_imputed['CUSTOMER_CODE'] = train_cdna_imputed['CUSTOMER_CODE']\n",
    "test_cdna_imputed['CUSTOMER_CODE'] = test_cdna_imputed['CUSTOMER_CODE']\n",
    "\n",
    "# Train join\n",
    "train_cdna_imputed['batch_date'] = pd.to_datetime(train_cdna_imputed['batch_date'])\n",
    "train_action['send_date'] = pd.to_datetime(train_action['send_timestamp'].dt.date)\n",
    "\n",
    "# Process train data in chunks\n",
    "chunk_size = 100000  # Adjust based on your RAM\n",
    "train_merged_chunks = []\n",
    "\n",
    "for start in range(0, len(train_action), chunk_size):\n",
    "    end = start + chunk_size\n",
    "    chunk = train_action.iloc[start:end].copy()\n",
    "    chunk['send_date'] = pd.to_datetime(chunk['send_timestamp'].dt.date)\n",
    "    \n",
    "    chunk_merged = pd.merge_asof(\n",
    "        chunk.sort_values('send_date'),\n",
    "        train_cdna_imputed.sort_values('batch_date'),\n",
    "        left_on='send_date',\n",
    "        right_on='batch_date',\n",
    "        left_by='customer_code',\n",
    "        right_by='CUSTOMER_CODE',\n",
    "        direction='backward'\n",
    "    )\n",
    "    train_merged_chunks.append(chunk_merged)\n",
    "    \n",
    "    # Free memory\n",
    "    del chunk_merged\n",
    "    del chunk\n",
    "    \n",
    "train_merged = pd.concat(train_merged_chunks)\n",
    "del train_merged_chunks\n",
    "\n",
    "# Test data (usually smaller, can process at once)\n",
    "test_merged = test_action.merge(\n",
    "    test_cdna_imputed,\n",
    "    left_on='customer_code',\n",
    "    right_on='CUSTOMER_CODE',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Train merged shape: {train_merged.shape}\")\n",
    "print(f\"Test merged shape: {test_merged.shape}\")\n",
    "\n",
    "# Validation checks\n",
    "def validate_merges():\n",
    "    print(\"Train Validation:\")\n",
    "    print(f\"Original shapes - Action: {train_action.shape}, CDNA: {train_cdna_imputed.shape}\")\n",
    "    print(f\"Merged shape: {train_merged.shape}\")\n",
    "    print(f\"Missing values in merged: {train_merged.isnull().sum().sum()}\")\n",
    "    print(f\"Unique customers in action: {train_action['customer_code'].nunique()}\")\n",
    "    print(f\"Unique customers in merged: {train_merged['customer_code'].nunique()}\")\n",
    "    \n",
    "    print(\"\\nTest Validation:\")\n",
    "    print(f\"Original shapes - Action: {test_action.shape}, CDNA: {test_cdna_imputed.shape}\")\n",
    "    print(f\"Merged shape: {test_merged.shape}\")\n",
    "    print(f\"Missing values in merged: {test_merged.isnull().sum().sum()}\")\n",
    "    print(f\"Unique customers in action: {test_action['customer_code'].nunique()}\")\n",
    "    print(f\"Unique customers in merged: {test_merged['customer_code'].nunique()}\")\n",
    "\n",
    "validate_merges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9178134-58c8-409c-978b-005e20f87d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in chunks to manage memory\n",
    "def save_in_chunks(df, filename, chunk_size=100000):\n",
    "    # First chunk with headers\n",
    "    df.iloc[:chunk_size].to_csv(filename, index=False)\n",
    "    \n",
    "    # Append remaining chunks without headers\n",
    "    for i in range(chunk_size, len(df), chunk_size):\n",
    "        df.iloc[i:i+chunk_size].to_csv(filename, mode='a', header=False, index=False)\n",
    "\n",
    "# Save train and test\n",
    "save_in_chunks(train_merged, 'train_merged.csv')\n",
    "save_in_chunks(test_merged, 'test_merged.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b6c3d7b-6550-4447-a5a6-c4e65b8f48a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dictionary changed size during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# List all variables in memory\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[0;32m      3\u001b[0m    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mglobals\u001b[39m()[obj], pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m      4\u001b[0m        \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mglobals\u001b[39m()[obj]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Memory usage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mglobals\u001b[39m()[obj]\u001b[38;5;241m.\u001b[39mmemory_usage()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: dictionary changed size during iteration"
     ]
    }
   ],
   "source": [
    "# List all variables in memory\n",
    "for obj in globals():\n",
    "   if isinstance(globals()[obj], pd.DataFrame):\n",
    "       print(f\"DataFrame '{obj}': {globals()[obj].shape}, Memory usage: {globals()[obj].memory_usage().sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e319f-3670-47a7-af65-1b5ea7b58b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_cdna_filtered\n",
    "del train_cdna_filtered\n",
    "del train_cdna_imputed\n",
    "del test_cdna_imputed\n",
    "del train_cdna\n",
    "del test_cdna\n",
    "del train_action\n",
    "del test_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3465c9-4c71-4992-8255-edba36c603c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check missing values by column\n",
    "missing_by_col = train_merged.isnull().sum()\n",
    "missing_pct_by_col = (train_merged.isnull().sum() / len(train_merged)) * 100\n",
    "\n",
    "# Create summary DataFrame\n",
    "missing_summary = pd.DataFrame({\n",
    "   'Missing Values': missing_by_col,\n",
    "   'Missing %': missing_pct_by_col\n",
    "})\n",
    "\n",
    "# Sort by most missing\n",
    "missing_summary = missing_summary[missing_summary['Missing Values'] > 0].sort_values('Missing Values', ascending=False)\n",
    "\n",
    "print(\"Train Missing Values Summary:\")\n",
    "print(missing_summary)\n",
    "\n",
    "# Same for test\n",
    "test_missing_by_col = test_merged.isnull().sum()\n",
    "test_missing_pct = (test_merged.isnull().sum() / len(test_merged)) * 100\n",
    "\n",
    "test_missing_summary = pd.DataFrame({\n",
    "   'Missing Values': test_missing_by_col,\n",
    "   'Missing %': test_missing_pct\n",
    "})\n",
    "\n",
    "test_missing_summary = test_missing_summary[test_missing_summary['Missing Values'] > 0].sort_values('Missing Values', ascending=False)\n",
    "\n",
    "print(\"\\nTest Missing Values Summary:\")\n",
    "print(test_missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c783c5-2b51-4b21-8011-bcbb95f627be",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01842238-777d-474e-96c7-74352392ccf5",
   "metadata": {},
   "source": [
    "- After this point, the 2 dataframes were downloaded as csvs, and uploaded again as dataframes due to recurring BSODs due to the large size of the dataframes. Currently, the code is continuing to use dataframes already present, however it has not been checked. Please run on your own discretion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819bc75-9bcc-45fe-8c03-d42c89d43d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d1014-ffb5-4bb9-bb0f-c8a1f09f79f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engagement metrics\n",
    "def add_engagement_metrics(df):\n",
    "   aggs = df.groupby('customer_code').agg(\n",
    "       open_rate=('is_open', 'mean'),\n",
    "       total_opens=('is_open', 'sum'),\n",
    "       total_emails=('is_open', 'count')\n",
    "   ).reset_index()\n",
    "   return df.merge(aggs, on='customer_code', how='left')\n",
    "\n",
    "train_merged = add_engagement_metrics(train_merged)\n",
    "test_merged = add_engagement_metrics(test_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2b468-edbc-46c7-9e7c-c03dbb6059df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_dataframe(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif df[col].dtype == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "        elif df[col].dtype == 'object':\n",
    "            if df[col].nunique() / len(df[col]) < 0.5:  # If low cardinality\n",
    "                df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "train_merged = optimize_dataframe(pd.read_csv('train_merged.csv'))\n",
    "test_merged = optimize_dataframe(pd.read_csv('test_merged.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7d965-452a-4ace-9fa7-fce147f962f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_customers = pd.read_csv('test_customers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bdf8b8-d4ed-43e2-92b4-5c9a879d0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "CATEGORICAL_COLS = [\n",
    "    'product_category', 'product_sub_category', 'send_slot', \n",
    "    'v6', 'v9', 'v10', 'v34', 'v35', 'v36', 'v37', 'v54', 'v101', 'v102'\n",
    "]\n",
    "\n",
    "NUMERICAL_COLS = [\n",
    "    'is_open', 'day_of_week', 'hour', 'send_slot_ML',\n",
    "    'v80', 'v287', 'v288', 'v289', 'v290', 'v291', 'v292', \n",
    "    'v293', 'v294', 'v295', 'v296', 'v297', 'v298', 'v299', \n",
    "    'v300', 'v301', 'v302'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a2a79d-e463-48da-9b42-1e0c7cf4bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df, encoders=None, scaler=None, is_training=True):\n",
    "    df = df.copy()\n",
    "    \n",
    "    if is_training:\n",
    "        encoders = {col: LabelEncoder() for col in CATEGORICAL_COLS}\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    for col in CATEGORICAL_COLS:\n",
    "        if df[col].dtype.name == 'category':\n",
    "            df[col] = df[col].astype(str)\n",
    "        df[col] = df[col].fillna('MISSING')\n",
    "        if is_training:\n",
    "            # Add 'MISSING' to training data before encoding\n",
    "            unique_vals = set(df[col].unique())\n",
    "            unique_vals.add('MISSING')\n",
    "            df[col] = df[col].astype(str)\n",
    "            encoders[col].fit(list(unique_vals))\n",
    "            df[col] = encoders[col].transform(df[col])\n",
    "        else:\n",
    "            known_categories = set(encoders[col].classes_)\n",
    "            df[col] = df[col].astype(str).apply(lambda x: x if x in known_categories else 'MISSING')\n",
    "            df[col] = encoders[col].transform(df[col])\n",
    "    \n",
    "    for col in NUMERICAL_COLS:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    if is_training:\n",
    "        df[NUMERICAL_COLS] = scaler.fit_transform(df[NUMERICAL_COLS])\n",
    "    else:\n",
    "        df[NUMERICAL_COLS] = scaler.transform(df[NUMERICAL_COLS])\n",
    "    \n",
    "    feature_cols = CATEGORICAL_COLS + NUMERICAL_COLS\n",
    "    X = df[feature_cols]\n",
    "    \n",
    "    if is_training:\n",
    "        return X, encoders, scaler\n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba9e3b-991e-488c-b0ce-1befe65b68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, num_slots=28):\n",
    "    \"\"\"\n",
    "    Neural network architecture optimized for slot prediction:\n",
    "    \n",
    "    Architecture:\n",
    "    - Input layer matching feature dimensions\n",
    "    - 3 Dense layers with decreasing sizes to create information bottleneck\n",
    "    - BatchNormalization after each Dense layer to stabilize learning\n",
    "    - Dropout layers with decreasing rates to prevent overfitting\n",
    "    - L2 regularization on first layer to control weight magnitudes\n",
    "    \n",
    "    Parameters:\n",
    "    - input_dim: Number of input features\n",
    "    - num_slots: Number of output slots (default 28)\n",
    "    \n",
    "    Key Design Choices:\n",
    "    1. Layer sizes (256->128->64): Creates gradual information bottleneck\n",
    "    2. Dropout rates (0.5->0.4->0.3): Higher dropout early, lower later\n",
    "    3. L2 regularization: Prevents any single feature from dominating\n",
    "    4. Learning rate 0.0003: Conservative to prevent overshooting\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.01), input_dim=input_dim),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(num_slots, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0003),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b427a3a-827c-4c3b-bb50-0e11ce5bf0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_predictions(predictions, slot_rankings):\n",
    "    \"\"\"\n",
    "    Validates prediction outputs for correctness and format\n",
    "    \n",
    "    Checks:\n",
    "    1. Correct number of slots (28)\n",
    "    2. No duplicate slots\n",
    "    3. Valid slot format (slot_X where X is 1-28)\n",
    "    4. Probability distribution sums to ~1\n",
    "    \"\"\"\n",
    "    for customer_predictions in slot_rankings:\n",
    "        if len(customer_predictions) != 28:\n",
    "            raise ValueError(f\"Each prediction must have 28 slots, got {len(customer_predictions)}\")\n",
    "        \n",
    "        if len(set(customer_predictions)) != 28:\n",
    "            raise ValueError(\"Predictions contain duplicate slots\")\n",
    "        \n",
    "        # Validate slot format\n",
    "        for slot in customer_predictions:\n",
    "            if not slot.startswith(\"slot_\"):\n",
    "                raise ValueError(f\"Invalid slot format: {slot}\")\n",
    "            try:\n",
    "                slot_num = int(slot.split('_')[1])\n",
    "                if not 1 <= slot_num <= 28:\n",
    "                    raise ValueError(f\"Invalid slot number: {slot_num}\")\n",
    "            except (IndexError, ValueError):\n",
    "                raise ValueError(f\"Invalid slot format: {slot}\")\n",
    "    \n",
    "    # Validate probabilities\n",
    "    if isinstance(predictions, np.ndarray):\n",
    "        prob_sums = np.sum(predictions, axis=1)\n",
    "        if not np.allclose(prob_sums, 1.0, atol=1e-6):\n",
    "            raise ValueError(\"Prediction probabilities do not sum to 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f57ecb-c0b4-4599-ac90-36381e43db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val, history):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with multiple metrics\n",
    "    \n",
    "    Metrics:\n",
    "    1. Accuracy (overall slot prediction accuracy)\n",
    "    2. Precision, Recall, F1 (per slot performance)\n",
    "    3. Training history analysis\n",
    "    4. Confusion patterns\n",
    "    \"\"\"\n",
    "    # Prediction metrics\n",
    "    predictions = model.predict(X_val)\n",
    "    y_pred = predictions.argmax(axis=1)\n",
    "    y_true = y_val.argmax(axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # Training history analysis\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    loss_diff = final_val_loss - final_train_loss\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'final_train_loss': final_train_loss,\n",
    "        'final_val_loss': final_val_loss,\n",
    "        'loss_difference': loss_diff\n",
    "    }\n",
    "    \n",
    "    logger.info(\"Model Evaluation Results:\")\n",
    "    for metric, value in metrics.items():\n",
    "        logger.info(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a2c12d-d99d-49c3-83be-6b4b88358068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_slots(slot_rankings):\n",
    "    if not slot_rankings:  # If no rankings yet, return sequential slots\n",
    "        return [f\"slot_{i+1}\" for i in range(28)]\n",
    "        \n",
    "    all_rankings = np.array([r for r in slot_rankings if len(r) == 28])\n",
    "    slot_freq = np.zeros((28, 28))\n",
    "    for ranking in all_rankings:\n",
    "        for pos, slot in enumerate(ranking):\n",
    "            slot_num = int(slot.split('_')[1]) - 1\n",
    "            slot_freq[pos, slot_num] += 1\n",
    "    \n",
    "    default_slots = []\n",
    "    used_slots = set()\n",
    "    for pos in range(28):\n",
    "        available_slots = [(i, slot_freq[pos, i]) for i in range(28) if i not in used_slots]\n",
    "        best_slot = max(available_slots, key=lambda x: x[1])[0]\n",
    "        used_slots.add(best_slot)\n",
    "        default_slots.append(f\"slot_{best_slot+1}\")\n",
    "    return default_slots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f893404-601a-4796-8c9c-f9f5809d8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(train_data, test_data, customer_codes):\n",
    "    # Prepare training data\n",
    "    X_train, encoders, scaler = prepare_features(train_data, is_training=True)\n",
    "    \n",
    "    # Create target variable (one-hot encoded slots)\n",
    "    # Shift slots down by 1 to handle slot_0\n",
    "    y_slots = train_data['send_slot_ML'].apply(lambda x: x - 1 if x > 0 else 27)\n",
    "    y_train = pd.get_dummies(y_slots, columns=range(28)).values\n",
    "    \n",
    "    # Split training data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Build and train model\n",
    "    model = build_model(X_train.shape[1])\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=1,\n",
    "        batch_size=1024,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Prepare test data\n",
    "    test_data = test_data[test_data['customer_code'].isin(test_customers['CUSTOMER_CODE'])]\n",
    "    X_test = prepare_features(test_data, encoders, scaler, is_training=False)\n",
    "    \n",
    "    # Get predictions\n",
    "    pred_probs = model.predict(X_test)\n",
    "    \n",
    "    # Average predictions per customer\n",
    "    customer_preds = {}\n",
    "    for i, customer in enumerate(test_data['customer_code']):\n",
    "        if customer not in customer_preds:\n",
    "            customer_preds[customer] = pred_probs[i]\n",
    "        else:\n",
    "            customer_preds[customer] += pred_probs[i]\n",
    "    \n",
    "    # Convert averaged predictions to rankings\n",
    "    slot_rankings = []\n",
    "    final_customers = []\n",
    "    \n",
    "    # Initialize default_slots with sequential order\n",
    "    default_slots = [f\"slot_{i+1}\" for i in range(28)]\n",
    "    \n",
    "    for customer_code in customer_codes:\n",
    "        if customer_code in customer_preds:\n",
    "            probs = customer_preds[customer_code]\n",
    "            ranked_slots = np.argsort(probs)[::-1]\n",
    "            slot_names = [f\"slot_{i+1}\" for i in ranked_slots]\n",
    "            slot_rankings.append(slot_names)\n",
    "            \n",
    "            if len(slot_rankings) % 1000 == 0:  # Update default slots periodically\n",
    "                default_slots = get_default_slots(slot_rankings)\n",
    "        else:\n",
    "            slot_rankings.append(default_slots)\n",
    "        final_customers.append(customer_code)\n",
    "\n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(model, X_val, y_val, history)\n",
    "    \n",
    "    # Get predictions\n",
    "    pred_probs = model.predict(X_test)\n",
    "    \n",
    "    # Validate predictions before creating rankings\n",
    "    validate_predictions(pred_probs, slot_rankings)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = pd.DataFrame({\n",
    "        'customer_code': final_customers,\n",
    "        'predicted_slots_order': slot_rankings\n",
    "    })\n",
    "    \n",
    "    return submission, history, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd90a7e-6245-4906-884c-9bd13e259a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train and predict\n",
    "submission, history, model = train_and_predict(\n",
    "    train_merged, test_merged, test_customers['CUSTOMER_CODE']\n",
    ")\n",
    "\n",
    "# 2. Save submission\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e57207-b635-4220-a449-816d193e4e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
